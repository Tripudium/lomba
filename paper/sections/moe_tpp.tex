\section{Mixture-of-exponentials temporal point processes}
\label{sec:moe-tpp}

Mixtures of exponentials offer a tractable family for modeling inter-arrival densities while retaining flexibility beyond a single-scale Poisson process. Let $K$ denote the number of components. Given non-negative weights $\alpha_k$ (not necessarily normalized) and rates $\beta_k>0$, the intensity and cumulative intensity after the last event are
\begin{align}
\lambda(\Delta t) &= \sum_{k=1}^K \alpha_k \beta_k \exp(-\beta_k \Delta t), \\
\Lambda(\Delta t) &= \sum_{k=1}^K \alpha_k \bigl(1 - e^{-\beta_k \Delta t}\bigr) / \beta_k.
\end{align}
The survival and horizon probabilities follow immediately:
\[
S(\Delta t) = \exp\bigl(-\Lambda(\Delta t)\bigr), \qquad
P(\Delta t \le \tau) = 1 - \exp\bigl(-\Lambda(\tau)\bigr).
\]
This yields closed-form log-likelihoods for observed arrivals ($\log \lambda - \Lambda$) and for censored observations ($-\Lambda$), making the family attractive for maximum-likelihood training and calibration at arbitrary horizons.

\paragraph{Connections to prior work.}
Mixture intensities and hazard mixtures have long been used in survival analysis as semi-parametric approximations \citep{daley2003introduction}. In neural point process literature, mixture or basis expansions of intensities provide a balance between flexibility and closed-form integration (e.g., \citealp{du2016recurrent,mei2017neural,tan2021relaxing}). Our choice mirrors this design: the model learns $(\alpha_k,\beta_k)$ from the Mamba hidden state via linear projections followed by softplus activations (to ensure positivity), preserving analytical integrals, inverse transform sampling, and efficient evaluation of $P(\Delta t \le \tau)$ needed for trading risk controls.
