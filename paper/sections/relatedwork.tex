\section{Related work}
\label{sec:related-work}

\paragraph{Neural temporal point processes.}
Recurrent and attention-based neural TPPs learn history-dependent intensities without hand-crafted kernels \citep{du2016recurrent,mei2017neural,shchur2021review,tan2021relaxing}. Many models trade off expressiveness and tractable integrals; basis expansions or mixtures (including exponentials) keep survival functions closed-form. Our approach uses a mixture-of-exponentials head to retain analytical likelihoods while delegating history modeling to a state-space backbone.

\paragraph{State-space sequence models.}
Selective state-space models such as Mamba \citep{gu2023mamba} provide linear-time sequence processing with competitive long-context capacity. Compared to transformers, they reduce quadratic cost and memory, making them attractive for long LOB streams. We combine Mamba with continuous-time conditioning to capture both inter-arrival structure and latent regime shifts (e.g., time-of-day).

\paragraph{Limit order book modeling.}
Deep models for LOB data have focused on price movement classification, depth forecasting, or point processes over order events. Prior work often uses Hawkes processes or RNN/transformer architectures with discrete time steps. By coupling a continuous-time head with hierarchical mark decoders, we aim to obtain calibrated arrival probabilities and structured mark distributions aligned with microstructure (spread-centered price bins, heavy-tailed sizes).
