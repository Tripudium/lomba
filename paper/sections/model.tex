\section{Model}

Our architecture combines engineered representations for LOB messages with a Mamba backbone and specialized output heads 
for time and marks.

\subsection{Feature extraction and binning}
We load raw LOB messages (time, type, order id, size, price, direction) and compute derived features $x_i$ 
for each event $i$:
\begin{itemize}[leftmargin=*,itemsep=0.25em]
    \item \textbf{Price change}: $\Delta p_i = (p_i - p_{i-1}) / \delta$, where $\delta$ is the tick size. This is discretized 
    into a centered window $[\ell, h]$ (e.g., $[-2, +2]$ ticks) plus two tail bins for larger moves.
    \item \textbf{Size and Time}: Log-transformed size $\log(1 + s_i)$ and inter-arrival time 
    $\log(1 + \Delta t_i / \tau_{\text{scale}})$, where $\tau_{\text{scale}}$ is the median positive inter-arrival 
    time in the training set.
    \item \textbf{Categorical}: Event type (limit, cancel, deletion, execution), side (ask/bid), and a level proxy 
    derived from $|\Delta p_i|$ (at-spread vs.\ away).
    \item \textbf{Time-of-day}: Absolute time $t_i^{\text{abs}}$ (seconds from midnight) is encoded cyclically as 
    \begin{equation*}
    \begin{aligned}
    [\sin(2\pi h/24), \cos(2\pi h/24), \sin(2\pi m/60), \cos(2\pi m/60)]
    \end{aligned}
    \end{equation*}
\end{itemize}

For continuous features (size, time), we employ a soft binning strategy to preserve local information. Given bin edges 
$e_0, \dots, e_B$, we compute the distance of a value $x$ to bin centers $c_j = (e_j + e_{j+1})/2$. A softmax with 
temperature $T$ yields weights $w_j \propto \exp(-|x - c_j|/T)$, which are used to compute a weighted sum of learnable bin 
embeddings $E \in \mathbb{R}^{B \times d}$:
\[
\mathbf{e}_{\text{soft}}(x) = \sum_{j=1}^B w_j(x) \mathbf{E}_j.
\]
This allows the model to interpolate between bins, mitigating boundary effects inherent in hard quantization.

\begin{table}[h]
\centering
\caption{Example of raw LOB messages and corresponding derived features.}
\label{tab:feature-example}
\small
\begin{tabular}{lcccccc}
\hline
\textbf{Field} & \textbf{Time} & \textbf{Type} & \textbf{Side} & \textbf{Price} & \textbf{Size} \\
\hline
Raw Value & $34200.123$ & 1 (Limit) & -1 (Ask) & $100.05$ & $100$ \\
Derived Feature & $t^{\text{abs}} = 34200.123$ & Type Code 0 & Side Code 0 & $\Delta p = 0$ ticks & $\log(1+s) \approx 4.62$ \\
& & & & & $\Delta t_{\text{prev}} = 0.050$s \\
\hline
\end{tabular}
\end{table}

For marks with meaningful locality (price, size, time), we structure the label space hierarchically. A field with $N$ 
fine bins is partitioned into $C$ coarse groups. For price, these groups typically represent "negative", "zero", and "positive" 
moves (or finer granularities like "small negative", "large negative"). Each fine bin $k$ maps to a coarse index $c(k)$ and a 
residual index $r(k)$. This structure informs both the model architecture and the loss function.

\subsection{Backbone}
For an input segment of length $L$, we embed each field and concatenate the embeddings. A learned projection maps the 
concatenated vector to the model dimension $d$. An additional projection of $\log(1+\Delta t_{i})$ injects inter-arrival 
information; cyclical encodings of absolute time-of-day are added when enabled. The resulting sequence is passed through $n$ 
Mamba layers, followed by LayerNorm and dropout.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mamba_model.png}
    \caption{Detailed view of the Mamba History Model. The architecture ingests a sequence of derived features, processes 
    them through a stack of Mamba layers, and branches into a temporal head (for arrival times) and hierarchical mark heads 
    (for event attributes).}
    \label{fig:mamba-detail}
\end{figure}

\subsection{Temporal head: mixture of exponentials}
Given hidden states $h_i \in \mathbb{R}^{d}$, the temporal head outputs non-negative weights and rates
$(\alpha_{i,k}, \beta_{i,k})_{k=1}^K$ via linear projections and softplus activations. Using~\eqref{eq:moe-intensity}, 
the log-likelihood for an observed $\Delta t_i$ is
\begin{equation}
\ell_{\text{time}} = \log \lambda(\Delta t_i) - \Lambda(\Delta t_i),
\end{equation}
while censored observations (no arrival before a cap) contribute $-\Lambda(\Delta t_{\max})$. The probability of at least 
one arrival within a horizon $\tau$ is $1 - e^{-\Lambda(\tau)}$, used for calibration and downstream risk flags.

\subsection{Hierarchical mark heads}
For each mark field, the prediction is factorized into a coarse selection and a residual refinement:
\[
p(m) = p(c \mid h_i) \, p(r \mid c, h_i).
\]
The coarse probability $p(c \mid h_i)$ is obtained via a linear layer and softmax. To condition the residual prediction on the coarse choice, we compute a context vector $\mathbf{c}_{\text{ctx}}$ by aggregating learnable coarse embeddings $\mathbf{E}^{\text{coarse}}$ weighted by the predicted coarse probabilities (soft conditioning):
\[
\mathbf{c}_{\text{ctx}} = \sum_{j} p(j \mid h_i) \mathbf{E}^{\text{coarse}}_j.
\]
The residual logits are then computed from the concatenation $[h_i; \mathbf{c}_{\text{ctx}}]$ via a specialized linear layer that outputs logits for all possible $(c, r)$ pairs, reshaped and masked to valid combinations. This design allows the model to share information across the coarse group while refining the specific bin prediction.

\subsection{Sampling}
Sampling a next event involves drawing $\Delta t$ from the mixture via inverse transform (with Newton refinement) 
and sampling marks by first drawing a coarse bin and then a residual, mapping back to fine indices. This allows joint 
Monte Carlo scenarios for stress testing or simulation.

